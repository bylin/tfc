## batch-effects
_Last modified `r format(Sys.time(), "%I:%M %p on %b %d, %Y")`. This document, R session image, knitr cache, figures, and other associated datasets are located in `cruncher:/inside/grotto/blin/trna-markers/batch-effects/`._

```{r setup, echo=FALSE, warning=FALSE, results=FALSE, message=FALSE, errors=FALSE, cache=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, cache.path="/inside/grotto/blin/trna-markers/batch-effects/cache/", eval=TRUE, echo=TRUE, warning=FALSE, results=FALSE, message=FALSE, autodep=TRUE, dev="png", dpi=300)
```

```{r libraries, cache=FALSE}
library(sva)
library(GenomicRanges)
library(DESeq2)
library(ggplot2)
library(BiocParallel)
register(MulticoreParam(8)) # turn this off depending on your machine!
attach('/inside/grotto/blin/trna-markers/feature-counts/feature-counts.RData')
```

TCGA data may contain batch effects that need to be removed. I normalize the data with [`DESeq2`](http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html) and use the [`sva`](http://www.bioconductor.org/packages/release/bioc/html/sva.html) package to remove them.

### Normalize counts
Let's load the `.RData` file and normalize the counts using `DESeq2`. For this first step, we will also eliminate features that have 0 counts for over 95% of the samples. This means that for a tRF or miRNA to be considered in the analysis, the feature must have at least 20 reads and be expressed in at least 5% of samples. These filters are stricter than usual because reads can be mapped to multiple loci, so a sample with 10 mapped locations may really just be the same read mapped 10 times.

```{r filter}
feature_counts <- feature_counts[apply(feature_counts, 1, function(row) length(which(row > 0)) > length(colnames(feature_counts)) * 0.05), ] # no. of samples with reads mapping to this feature > no. samples * 0.05
feature_counts <- feature_counts[apply(feature_counts, 1, function(row) sum(row) > 20), ]
```

```{r get-scaling-factors}
colData <- data.frame(row.names = colnames(feature_counts), condition = prad_metadata$sample_type, type = "single-read") # feature count column names correspond to metadata rows
dds <- DESeqDataSetFromMatrix(countData = feature_counts, colData = colData, design = ~ condition)
dds <- estimateSizeFactors(dds)
```

We don't need the entire DESeq analysis pipeline. The only thing of interest is the scaling factors which we can now apply to our hg19 feature counts.

```{r normalize}
normalized_counts <- t(t(feature_counts) / sizeFactors(dds)) # R applies vectors on columns (genes), we want R to apply them on rows (samples) for size factors
```

## Surrogate variable analysis
```{r estimate-surrogate-variables}
pheno_data <- data.frame(row.names = prad_metadata$barcode, sample_type = prad_metadata$sample_type)
mod <- model.matrix(~ as.factor(sample_type), data = pheno_data)
mod0 = model.matrix(~ 1, data = pheno_data)
svobj <- sva(dat = normalized_counts, mod = mod, mod0 = mod0)
```

```{r remove-batch-function}
# No idea how this works. Lifted off of https://www.biostars.org/p/121489/
removeBatchEffect <- function(counts, mod, svs) {
  x = cbind(mod, svs)
  hat = solve(t(x) %*% x) %*% t(x)
  beta = (hat %*% t(counts))
  p = ncol(mod)
  return(counts - t(as.matrix(x[,-c(1:p)]) %*% beta[-c(1:p),]))
}
```

```{r remove-batch}
batch_removed_counts <- removeBatchEffect(normalized_counts, mod, svobj$sv)
```

## PCA
To look at the effectiveness of ComBat, I will project pre- and post-ComBat expression data onto its principle components.

```{r pca, fig.show = "hold", out.width = "400px", fig.width = 4, fig.height = 3}
plotPCA <- function(counts) {
  pca <- prcomp(counts, scale = TRUE)
  pca_scores <- as.data.frame(pca$x)
  ggplot(pca_scores, aes(x = PC1, y = PC2)) + geom_point(shape = 1)
}
plotPCACancerType <- function(normalized_counts, adjusted_counts, metadata, cancer_code) {
	plotPCA(t(normalized_counts)[, metadata$sample_type == "TP"]) + ggtitle(paste(cancer_code, "TP Before"))
	plotPCA(t(adjusted_counts)[, metadata$sample_type == "TP"]) + ggtitle(paste(cancer_code, "TP After"))
	plotPCA(t(normalized_counts)[, metadata$sample_type == "NT"]) + ggtitle(paste(cancer_code, "NT Before"))
	plotPCA(t(adjusted_counts)[, metadata$sample_type == "NT"]) + ggtitle(paste(cancer_code, "NT After"))
}
```

```{r pca-all, fig.show = "hold", out.width = "400px", fig.width = 4, fig.height = 3, fig.cap = "Before and after PCA plots"}
# separate by tissue type - otherwise will just cluster by cancer incidence
pca <- as.data.frame(prcomp(t(normalized_counts), scale = TRUE)$x)
pca$sample_type <- prad_metadata$sample_type
ggplot(pca, aes(x = PC1, y = PC2, color = sample_type)) + geom_point(shape = 1) + ggtitle("Before")
pca <- as.data.frame(prcomp(t(batch_removed_counts), scale = TRUE)$x)
pca$sample_type <- prad_metadata$sample_type
ggplot(pca, aes(x = PC1, y = PC2, color = sample_type)) + geom_point(shape = 1) + ggtitle("After")
```

It doesn't look like removing surrogate variables is achieving a lot. I will just use the normalized counts instead.

```{r save-image}
save(file = "counts.RData", batch_removed_counts, normalized_counts)
save.image("batch-effects-image.RData")
```
